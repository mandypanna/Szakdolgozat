{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XpUVncB_U9Sn"
      },
      "outputs": [],
      "source": [
        "!wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7-tiny.pt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8Y3ox9WyLWo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "os.chdir(\"/content/drive/My Drive/\")\n",
        "\n",
        "os.chdir('yolov7')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eqNgn5tjVbkY"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/yolov7/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chEpjtEGVgJO"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from utils.datasets import letterbox\n",
        "from utils.general import non_max_suppression, scale_coords\n",
        "from utils.plots import plot_one_box\n",
        "import imutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NVKItDWLXHds"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "from models.common import Conv, DWConv\n",
        "from utils.google_utils import attempt_download\n",
        "\n",
        "\n",
        "class CrossConv(nn.Module):\n",
        "    def __init__(self, c1, c2, k=3, s=1, g=1, e=1.0, shortcut=False):\n",
        "        super(CrossConv, self).__init__()\n",
        "        c_ = int(c2 * e)\n",
        "        self.cv1 = Conv(c1, c_, (1, k), (1, s))\n",
        "        self.cv2 = Conv(c_, c2, (k, 1), (s, 1), g=g)\n",
        "        self.add = shortcut and c1 == c2\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
        "\n",
        "\n",
        "class Sum(nn.Module):\n",
        "    def __init__(self, n, weight=False):\n",
        "        super(Sum, self).__init__()\n",
        "        self.weight = weight\n",
        "        self.iter = range(n - 1)\n",
        "        if weight:\n",
        "            self.w = nn.Parameter(-torch.arange(1., n) / 2, requires_grad=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x[0]\n",
        "        if self.weight:\n",
        "            w = torch.sigmoid(self.w) * 2\n",
        "            for i in self.iter:\n",
        "                y = y + x[i + 1] * w[i]\n",
        "        else:\n",
        "            for i in self.iter:\n",
        "                y = y + x[i + 1]\n",
        "        return y\n",
        "\n",
        "\n",
        "class MixConv2d(nn.Module):\n",
        "    def __init__(self, c1, c2, k=(1, 3), s=1, equal_ch=True):\n",
        "        super(MixConv2d, self).__init__()\n",
        "        groups = len(k)\n",
        "        if equal_ch:\n",
        "            i = torch.linspace(0, groups - 1E-6, c2).floor()\n",
        "            c_ = [(i == g).sum() for g in range(groups)]\n",
        "        else:\n",
        "            b = [c2] + [0] * groups\n",
        "            a = np.eye(groups + 1, groups, k=-1)\n",
        "            a -= np.roll(a, 1, axis=1)\n",
        "            a *= np.array(k) ** 2\n",
        "            a[0] = 1\n",
        "            c_ = np.linalg.lstsq(a, b, rcond=None)[0].round()\n",
        "\n",
        "        self.m = nn.ModuleList([nn.Conv2d(c1, int(c_[g]), k[g], s, k[g] // 2, bias=False) for g in range(groups)])\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = nn.LeakyReLU(0.1, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.act(self.bn(torch.cat([m(x) for m in self.m], 1)))\n",
        "\n",
        "\n",
        "class Ensemble(nn.ModuleList):\n",
        "    def __init__(self):\n",
        "        super(Ensemble, self).__init__()\n",
        "\n",
        "    def forward(self, x, augment=False):\n",
        "        y = []\n",
        "        for module in self:\n",
        "            y.append(module(x, augment)[0])\n",
        "        y = torch.cat(y, 1)\n",
        "        return y, None\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "class ORT_NMS(torch.autograd.Function):\n",
        "    '''ONNX-Runtime NMS operation'''\n",
        "    @staticmethod\n",
        "    def forward(ctx,\n",
        "                boxes,\n",
        "                scores,\n",
        "                max_output_boxes_per_class=torch.tensor([100]),\n",
        "                iou_threshold=torch.tensor([0.45]),\n",
        "                score_threshold=torch.tensor([0.25])):\n",
        "        device = boxes.device\n",
        "        batch = scores.shape[0]\n",
        "        num_det = random.randint(0, 100)\n",
        "        batches = torch.randint(0, batch, (num_det,)).sort()[0].to(device)\n",
        "        idxs = torch.arange(100, 100 + num_det).to(device)\n",
        "        zeros = torch.zeros((num_det,), dtype=torch.int64).to(device)\n",
        "        selected_indices = torch.cat([batches[None], zeros[None], idxs[None]], 0).T.contiguous()\n",
        "        selected_indices = selected_indices.to(torch.int64)\n",
        "        return selected_indices\n",
        "\n",
        "    @staticmethod\n",
        "    def symbolic(g, boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold):\n",
        "        return g.op(\"NonMaxSuppression\", boxes, scores, max_output_boxes_per_class, iou_threshold, score_threshold)\n",
        "\n",
        "\n",
        "class TRT_NMS(torch.autograd.Function):\n",
        "    '''TensorRT NMS operation'''\n",
        "    @staticmethod\n",
        "    def forward(\n",
        "        ctx,\n",
        "        boxes,\n",
        "        scores,\n",
        "        background_class=-1,\n",
        "        box_coding=1,\n",
        "        iou_threshold=0.45,\n",
        "        max_output_boxes=100,\n",
        "        plugin_version=\"1\",\n",
        "        score_activation=0,\n",
        "        score_threshold=0.25,\n",
        "    ):\n",
        "        batch_size, num_boxes, num_classes = scores.shape\n",
        "        num_det = torch.randint(0, max_output_boxes, (batch_size, 1), dtype=torch.int32)\n",
        "        det_boxes = torch.randn(batch_size, max_output_boxes, 4)\n",
        "        det_scores = torch.randn(batch_size, max_output_boxes)\n",
        "        det_classes = torch.randint(0, num_classes, (batch_size, max_output_boxes), dtype=torch.int32)\n",
        "        return num_det, det_boxes, det_scores, det_classes\n",
        "\n",
        "    @staticmethod\n",
        "    def symbolic(g,\n",
        "                 boxes,\n",
        "                 scores,\n",
        "                 background_class=-1,\n",
        "                 box_coding=1,\n",
        "                 iou_threshold=0.45,\n",
        "                 max_output_boxes=100,\n",
        "                 plugin_version=\"1\",\n",
        "                 score_activation=0,\n",
        "                 score_threshold=0.25):\n",
        "        out = g.op(\"TRT::EfficientNMS_TRT\",\n",
        "                   boxes,\n",
        "                   scores,\n",
        "                   background_class_i=background_class,\n",
        "                   box_coding_i=box_coding,\n",
        "                   iou_threshold_f=iou_threshold,\n",
        "                   max_output_boxes_i=max_output_boxes,\n",
        "                   plugin_version_s=plugin_version,\n",
        "                   score_activation_i=score_activation,\n",
        "                   score_threshold_f=score_threshold,\n",
        "                   outputs=4)\n",
        "        nums, boxes, scores, classes = out\n",
        "        return nums, boxes, scores, classes\n",
        "\n",
        "\n",
        "class ONNX_ORT(nn.Module):\n",
        "    '''onnx module with ONNX-Runtime NMS operation.'''\n",
        "    def __init__(self, max_obj=100, iou_thres=0.45, score_thres=0.25, max_wh=640, device=None, n_classes=80):\n",
        "        super().__init__()\n",
        "        self.device = device if device else torch.device(\"cpu\")\n",
        "        self.max_obj = torch.tensor([max_obj]).to(device)\n",
        "        self.iou_threshold = torch.tensor([iou_thres]).to(device)\n",
        "        self.score_threshold = torch.tensor([score_thres]).to(device)\n",
        "        self.max_wh = max_wh\n",
        "        self.convert_matrix = torch.tensor([[1, 0, 1, 0], [0, 1, 0, 1], [-0.5, 0, 0.5, 0], [0, -0.5, 0, 0.5]],\n",
        "                                           dtype=torch.float32,\n",
        "                                           device=self.device)\n",
        "        self.n_classes=n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        boxes = x[:, :, :4]\n",
        "        conf = x[:, :, 4:5]\n",
        "        scores = x[:, :, 5:]\n",
        "        if self.n_classes == 1:\n",
        "            scores = conf\n",
        "        else:\n",
        "            scores *= conf\n",
        "        boxes @= self.convert_matrix\n",
        "        max_score, category_id = scores.max(2, keepdim=True)\n",
        "        dis = category_id.float() * self.max_wh\n",
        "        nmsbox = boxes + dis\n",
        "        max_score_tp = max_score.transpose(1, 2).contiguous()\n",
        "        selected_indices = ORT_NMS.apply(nmsbox, max_score_tp, self.max_obj, self.iou_threshold, self.score_threshold)\n",
        "        X, Y = selected_indices[:, 0], selected_indices[:, 2]\n",
        "        selected_boxes = boxes[X, Y, :]\n",
        "        selected_categories = category_id[X, Y, :].float()\n",
        "        selected_scores = max_score[X, Y, :]\n",
        "        X = X.unsqueeze(1).float()\n",
        "        return torch.cat([X, selected_boxes, selected_categories, selected_scores], 1)\n",
        "\n",
        "class ONNX_TRT(nn.Module):\n",
        "    def __init__(self, max_obj=100, iou_thres=0.45, score_thres=0.25, max_wh=None ,device=None, n_classes=80):\n",
        "        super().__init__()\n",
        "        assert max_wh is None\n",
        "        self.device = device if device else torch.device('cpu')\n",
        "        self.background_class = -1,\n",
        "        self.box_coding = 1,\n",
        "        self.iou_threshold = iou_thres\n",
        "        self.max_obj = max_obj\n",
        "        self.plugin_version = '1'\n",
        "        self.score_activation = 0\n",
        "        self.score_threshold = score_thres\n",
        "        self.n_classes=n_classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        boxes = x[:, :, :4]\n",
        "        conf = x[:, :, 4:5]\n",
        "        scores = x[:, :, 5:]\n",
        "        if self.n_classes == 1:\n",
        "            scores = conf\n",
        "        else:\n",
        "            scores *= conf\n",
        "        num_det, det_boxes, det_scores, det_classes = TRT_NMS.apply(boxes, scores, self.background_class, self.box_coding,\n",
        "                                                                    self.iou_threshold, self.max_obj,\n",
        "                                                                    self.plugin_version, self.score_activation,\n",
        "                                                                    self.score_threshold)\n",
        "        return num_det, det_boxes, det_scores, det_classes\n",
        "\n",
        "\n",
        "class End2End(nn.Module):\n",
        "    def __init__(self, model, max_obj=100, iou_thres=0.45, score_thres=0.25, max_wh=None, device=None, n_classes=80):\n",
        "        super().__init__()\n",
        "        device = device if device else torch.device('cpu')\n",
        "        assert isinstance(max_wh,(int)) or max_wh is None\n",
        "        self.model = model.to(device)\n",
        "        self.model.model[-1].end2end = True\n",
        "        self.patch_model = ONNX_TRT if max_wh is None else ONNX_ORT\n",
        "        self.end2end = self.patch_model(max_obj, iou_thres, score_thres, max_wh, device, n_classes)\n",
        "        self.end2end.eval()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        x = self.end2end(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def attempt_load(weights, map_location=None, inplace=True, fuse=True):\n",
        "    model = []\n",
        "    for w in weights if isinstance(weights, list) else [weights]:\n",
        "        attempt_download(w)\n",
        "        ckpt = torch.load(w, map_location=map_location, weights_only=False)\n",
        "        model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())\n",
        "    return model[0] if len(model) == 1 else model\n",
        "    for m in model.modules():\n",
        "        if type(m) in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU]:\n",
        "            m.inplace = True\n",
        "        elif type(m) is nn.Upsample:\n",
        "            m.recompute_scale_factor = None\n",
        "        elif type(m) is Conv:\n",
        "            m._non_persistent_buffers_set = set()\n",
        "\n",
        "    if len(model) == 1:\n",
        "        return model[-1]\n",
        "    else:\n",
        "        print('Ensemble created with %s\\n' % weights)\n",
        "        for k in ['names', 'stride']:\n",
        "            setattr(model, k, getattr(model[-1], k))\n",
        "        return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GegWrt6nWf5w"
      },
      "outputs": [],
      "source": [
        "yolov7_path = \"./yolov7.pt\"\n",
        "yolov7_tiny_path = \"./yolov7-tiny.pt\"\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model_v7 = attempt_load(yolov7_path, map_location=device)\n",
        "model_tiny = attempt_load(yolov7_tiny_path, map_location=device)\n",
        "\n",
        "model_v7.eval()\n",
        "model_tiny.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_6zgpjl-Reuk"
      },
      "outputs": [],
      "source": [
        "def draw_boxes(image, detections, label=\"\"):\n",
        "    for *xyxy, conf, cls in detections:\n",
        "        x1, y1, x2, y2 = map(int, xyxy)\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        cv2.putText(image, f\"{label} {conf:.2f}\", (x1, y1 - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 1)\n",
        "    return image\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FjAHD1CCXzHp"
      },
      "outputs": [],
      "source": [
        "\n",
        "def yolov7onframe_onlyPerson(frame):\n",
        "    ll = \"-1\"\n",
        "    rxyxy = \"-1\"\n",
        "\n",
        "    try:\n",
        "      img = letterbox(frame, new_shape=640)[0]\n",
        "      img = img[:, :, ::-1].transpose(2, 0, 1)\n",
        "      img = np.ascontiguousarray(img)\n",
        "\n",
        "      img = torch.from_numpy(img).to(device)\n",
        "      img = img.float()\n",
        "      img /= 255.0\n",
        "      if img.ndimension() == 3:\n",
        "          img = img.unsqueeze(0)\n",
        "\n",
        "      with torch.no_grad():\n",
        "          pred = model_tiny(img)[0]\n",
        "      pred = non_max_suppression(pred, 0.5, 0.45, classes=None, agnostic=False)\n",
        "\n",
        "      for i, det in enumerate(pred):\n",
        "          if len(det):\n",
        "              det[:, :4] = scale_coords(img.shape[2:], det[:, :4], frame.shape).round()\n",
        "\n",
        "              for *xyxy, conf, cls in reversed(det):\n",
        "                  label = f'{model.names[int(cls)]} {conf:.2f}'\n",
        "                  if(\"person\" in label):\n",
        "                    print(label)\n",
        "                    ll = label\n",
        "                    rxyxy = xyxy\n",
        "                    print(*xyxy)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "\n",
        "    return ll,frame,rxyxy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OimcJ-_l4vFd"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from scipy.fft import fft, fftfreq\n",
        "\n",
        "def dominant_frequency(signal, fps=25):\n",
        "    N = len(signal)\n",
        "    yf = fft(signal)\n",
        "    xf = fftfreq(N, 1 / fps)[:N // 2]\n",
        "\n",
        "    amplitudes = np.abs(yf[:N // 2])\n",
        "    dominant_freq = xf[np.argmax(amplitudes)]\n",
        "\n",
        "    return dominant_freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I44I-Xvw22cn"
      },
      "outputs": [],
      "source": [
        "def convert_xyxy_to_bbox(xyxy):\n",
        "    x1, y1, x2, y2 = [int(val.item()) for val in xyxy]\n",
        "    return (x1, y1, x2 - x1, y2 - y1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lx6C8YCs8zAa"
      },
      "outputs": [],
      "source": [
        "indexes = []\n",
        "is_coord = []\n",
        "coordinatess = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OAuidyVR0r1I"
      },
      "outputs": [],
      "source": [
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/baba_legzes_2.mp4\")\n",
        "length = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "print( length )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NJrf27ZaQ-1"
      },
      "outputs": [],
      "source": [
        "frames = []\n",
        "for i in range(1500):\n",
        "    ret, frame = cap.read()\n",
        "    if(ret==True):\n",
        "      frames.append(frame)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oNte_FZfUW-X"
      },
      "outputs": [],
      "source": [
        "import time\n",
        "import numpy as np\n",
        "\n",
        "def average_vector(flow):\n",
        "    return [sum(sum(flow))/len(flow)][0]\n",
        "\n",
        "\n",
        "def calc_rep(flow):\n",
        "    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    return np.mean(magnitude)\n",
        "\n",
        "\n",
        "def calcLenght(flow):\n",
        "  return sum(sum(flow[:,:,0]*flow[:,:,0]))+sum(sum(flow[:,:,1]*flow[:,:,1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPHZ7C_zB4Xh"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/baba_legzes_2.mp4\")\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "print(f\"FPS: {fps}\")\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WN9Z8ZtyYJ1w"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "\n",
        "def calc_rep(flow):\n",
        "    magnitude, _ = cv2.cartToPolar(flow[..., 0], flow[..., 1])\n",
        "    return np.mean(magnitude)\n",
        "\n",
        "blur_kernel_size = (5, 5)\n",
        "motion_amplitudes = []\n",
        "\n",
        "frame1 = frames[0][300:900, 300:1000]\n",
        "frame1 = cv2.resize(frame1, (500, 500))\n",
        "prvs = cv2.cvtColor(frame1, cv2.COLOR_BGR2GRAY)\n",
        "prvs = cv2.GaussianBlur(prvs, blur_kernel_size, 0)\n",
        "\n",
        "for i in range(1, len(frames)):\n",
        "    frame = frames[i][300:900, 300:1000]\n",
        "    frame = cv2.resize(frame, (500, 500))\n",
        "    next_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
        "    next_frame = cv2.GaussianBlur(next_frame, blur_kernel_size, 0)\n",
        "\n",
        "    flow = cv2.calcOpticalFlowFarneback(prvs, next_frame, None, 0.5, 3, 15, 3, 5, 1.2, 0)\n",
        "    motion_amplitudes.append(calc_rep(flow))\n",
        "\n",
        "    prvs = next_frame\n",
        "\n",
        "print(\" Mozgásdetekció kész. Értékek száma:\", len(motion_amplitudes))\n",
        "\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(motion_amplitudes)\n",
        "plt.title(\"Mozgási amplitúdó görbe\")\n",
        "plt.xlabel(\"Képkocka index\")\n",
        "plt.ylabel(\"Mozgás mértéke\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install ftfy regex tqdm\n",
        "!pip install git+https://github.com/openai/CLIP.git\n",
        "\n",
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n"
      ],
      "metadata": {
        "id": "JQWZi_MA95ux"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import time\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def run_yolo(model, image):\n",
        "    img = letterbox(image, new_shape=640)[0]\n",
        "    img = img[:, :, ::-1].transpose(2, 0, 1)\n",
        "    img = np.ascontiguousarray(img)\n",
        "    img_tensor = torch.from_numpy(img).to(device).float() / 255.0\n",
        "    if img_tensor.ndimension() == 3:\n",
        "        img_tensor = img_tensor.unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        start = time.time()\n",
        "        pred = model(img_tensor)[0]\n",
        "        inference_time = time.time() - start\n",
        "\n",
        "        pred = non_max_suppression(pred, 0.05, 0.45)\n",
        "        det = pred[0]\n",
        "\n",
        "        if det is not None and len(det):\n",
        "            det[:, :4] = scale_coords(img_tensor.shape[2:], det[:, :4], image.shape).round()\n",
        "        return det.cpu().numpy() if det is not None else [], inference_time\n",
        "\n",
        "\n",
        "def draw_boxes(image, detections, label=\"Model\"):\n",
        "    for *xyxy, conf, cls in detections:\n",
        "        x1, y1, x2, y2 = map(int, xyxy)\n",
        "        cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "        cv2.putText(image, f\"{label} {conf:.2f}\", (x1, y1 - 10),\n",
        "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 1)\n",
        "    return image\n",
        "\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/baba_legzes_2.mp4\")\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "output_size = (width * 2, height)\n",
        "out_video = cv2.VideoWriter(\"yolo_baba_1_hosszu.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), fps, output_size)\n",
        "\n",
        "frame_idx = 0\n",
        "frame_limit = 100\n",
        "\n",
        "while cap.isOpened() and frame_idx < frame_limit:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    print(f\"\\n Frame {frame_idx}:\")\n",
        "\n",
        "    det_v7, _ = run_yolo(model_v7, frame)\n",
        "    det_tiny, _ = run_yolo(model_tiny, frame)\n",
        "\n",
        "    conf_v7 = np.mean([d[4] for d in det_v7]) if len(det_v7) else 0\n",
        "    conf_tiny = np.mean([d[4] for d in det_tiny]) if len(det_tiny) else 0\n",
        "\n",
        "    print(f\"YOLOv7 - Conf: {conf_v7:.2f}, Detections: {len(det_v7)}\")\n",
        "    print(f\"YOLOv7-Tiny - Conf: {conf_tiny:.2f}, Detections: {len(det_tiny)}\")\n",
        "\n",
        "    best_v7 = [max(det_v7, key=lambda d: d[4])] if len(det_v7) else []\n",
        "    img_v7 = draw_boxes(frame.copy(), best_v7, \"YOLOv7\")\n",
        "\n",
        "    best_tiny = [max(det_tiny, key=lambda d: d[4])] if len(det_tiny) else []\n",
        "    img_tiny = draw_boxes(frame.copy(), best_tiny, \"YOLOv7-Tiny\")\n",
        "\n",
        "    combined = np.hstack((img_v7, img_tiny))\n",
        "\n",
        "    out_video.write(combined)\n",
        "\n",
        "    frame_idx += 1\n",
        "\n",
        "\n",
        "cap.release()\n",
        "out_video.release()\n",
        "print(\" Kimeneti videó elkészült\")\n",
        "\n",
        "det_v7, time_v7 = run_yolo(model_v7, frame)\n",
        "gpu_v7 = torch.cuda.memory_allocated() / 1e6 if torch.cuda.is_available() else 0\n",
        "conf_v7 = np.mean([d[4] for d in det_v7]) if len(det_v7) else 0\n",
        "\n",
        "det_tiny, time_tiny = run_yolo(model_tiny, frame)\n",
        "gpu_tiny = torch.cuda.memory_allocated() / 1e6 if torch.cuda.is_available() else 0\n",
        "conf_tiny = np.mean([d[4] for d in det_tiny]) if len(det_tiny) else 0\n",
        "\n",
        "print(f\"YOLOv7:\")\n",
        "print(f\" - Inference time: {time_v7:.4f}s\")\n",
        "print(f\" - Detections: {len(det_v7)}\")\n",
        "print(f\" - Avg confidence: {conf_v7:.2f}\")\n",
        "print(f\" - GPU memory used: {gpu_v7:.2f} MB\\n\")\n",
        "\n",
        "print(f\"YOLOv7-Tiny:\")\n",
        "print(f\" - Inference time: {time_tiny:.4f}s\")\n",
        "print(f\" - Detections: {len(det_tiny)}\")\n",
        "print(f\" - Avg confidence: {conf_tiny:.2f}\")\n",
        "print(f\" - GPU memory used: {gpu_tiny:.2f} MB\\n\")\n",
        "\n",
        "best_v7 = [max(det_v7, key=lambda d: d[4])] if len(det_v7) else []\n",
        "best_tiny = [max(det_tiny, key=lambda d: d[4])] if len(det_tiny) else []\n",
        "\n",
        "img_v7 = draw_boxes(frame.copy(), best_v7, \"YOLOv7\")\n",
        "img_tiny = draw_boxes(frame.copy(), best_tiny, \"YOLOv7-Tiny\")\n",
        "\n",
        "plt.figure(figsize=(16, 7))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title(\"YOLOv7\")\n",
        "plt.imshow(cv2.cvtColor(img_v7, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title(\"YOLOv7-Tiny\")\n",
        "plt.imshow(cv2.cvtColor(img_tiny, cv2.COLOR_BGR2RGB))\n",
        "plt.axis(\"off\")\n",
        "\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "nRNqBVHf1Sfd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import clip\n",
        "import torch\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from datetime import timedelta\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
        "\n",
        "clip_prompts = [\n",
        "    \"a baby alone in an incubator\",\n",
        "    \"a baby with a nurse's hand nearby\",\n",
        "    \"a baby being touched by a hand\",\n",
        "    \"a baby lying without anyone nearby\"\n",
        "]\n",
        "clip_texts = clip.tokenize(clip_prompts).to(device)\n",
        "\n",
        "def clip_detect_hand_near_baby(cropped_baby_frame, threshold=0.3):\n",
        "    image = preprocess(Image.fromarray(cropped_baby_frame)).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "        image_features = clip_model.encode_image(image)\n",
        "        image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        text_features = clip_model.encode_text(clip_texts)\n",
        "        text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        similarity = (image_features @ text_features.T).squeeze(0)\n",
        "        best_match_idx = similarity.argmax().item()\n",
        "        best_score = similarity[best_match_idx].item()\n",
        "\n",
        "        return \"hand\" in clip_prompts[best_match_idx] and best_score > threshold\n",
        "\n",
        "def group_time_ranges(frame_statuses, fps):\n",
        "    intervals = []\n",
        "    if not frame_statuses:\n",
        "        return intervals\n",
        "\n",
        "    current_status = frame_statuses[0][1]\n",
        "    start_frame = frame_statuses[0][0]\n",
        "\n",
        "    for i in range(1, len(frame_statuses)):\n",
        "        frame_idx, status = frame_statuses[i]\n",
        "        if status != current_status:\n",
        "            intervals.append((start_frame, frame_statuses[i - 1][0], current_status))\n",
        "            start_frame = frame_idx\n",
        "            current_status = status\n",
        "    intervals.append((start_frame, frame_statuses[-1][0], current_status))\n",
        "    return intervals\n",
        "\n",
        "def format_time(seconds):\n",
        "    return str(timedelta(seconds=int(seconds)))\n",
        "\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/baba_legzes_2.mp4\")\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "frame_idx = 0\n",
        "frame_statuses = []\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    detections, _ = run_yolo(model_v7, frame)\n",
        "    baby_class = 0\n",
        "    baby_found = False\n",
        "    status = \"nincs_baba\"\n",
        "\n",
        "    for det in detections:\n",
        "        x1, y1, x2, y2, conf, cls = det\n",
        "        if int(cls) == baby_class:\n",
        "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "            baby_crop = frame[y1:y2, x1:x2]\n",
        "            if baby_crop.size == 0:\n",
        "                continue\n",
        "\n",
        "            baby_found = True\n",
        "            has_hand = clip_detect_hand_near_baby(baby_crop)\n",
        "\n",
        "            status = \"kéz\" if has_hand else \"tiszta\"\n",
        "            break\n",
        "\n",
        "    if not baby_found:\n",
        "        status = \"nincs_baba\"\n",
        "\n",
        "    frame_statuses.append((frame_idx, status))\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release()\n",
        "\n",
        "print(\"\\n Kéz jelenlétének időszakaszai:\")\n",
        "\n",
        "if frame_statuses:\n",
        "    prev_status = frame_statuses[0][1]\n",
        "    start_f = frame_statuses[0][0]\n",
        "\n",
        "    for i in range(1, len(frame_statuses)):\n",
        "        frame_idx, status = frame_statuses[i]\n",
        "        if status != prev_status:\n",
        "            start_time = str(timedelta(seconds=int(start_f // fps)))\n",
        "            end_time = str(timedelta(seconds=int(frame_statuses[i - 1][0] // fps)))\n",
        "\n",
        "            if prev_status == \"kéz\":\n",
        "                print(f\" Kéz a képen: {start_time} - {end_time}\")\n",
        "            elif prev_status == \"tiszta\":\n",
        "                print(f\" Tiszta időszak: {start_time} - {end_time}\")\n",
        "            else:\n",
        "                print(f\" Baba nem látszik: {start_time} - {end_time}\")\n",
        "\n",
        "            start_f = frame_idx\n",
        "            prev_status = status\n",
        "\n",
        "    # utolsó szakasz kiírása\n",
        "    start_time = str(timedelta(seconds=int(start_f // fps)))\n",
        "    end_time = str(timedelta(seconds=int(frame_statuses[-1][0] // fps)))\n",
        "    if prev_status == \"kéz\":\n",
        "        print(f\" Kéz a képen: {start_time} - {end_time}\")\n",
        "    elif prev_status == \"tiszta\":\n",
        "        print(f\" Tiszta időszak: {start_time} - {end_time}\")\n",
        "    else:\n",
        "        print(f\" Baba nem látszik: {start_time} - {end_time}\")\n"
      ],
      "metadata": {
        "id": "qCbnNOWN0Xrs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "additional_prompts = [\n",
        "    \"a hand reaching into the incubator\",\n",
        "    \"a nurse touching the baby\",\n",
        "    \"a human hand near an infant\",\n",
        "    \"a caregiver hand over the baby\",\n",
        "    \"a person placing hand on baby\",\n",
        "    \"a nurse placing hand close to a baby\"\n",
        "]\n",
        "\n",
        "kezmentes_intervallumok = [(0, 133), (262, 267), (332, 575)]\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/baba_legzes_2.mp4\")\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "cap.release()\n",
        "\n",
        "def is_frame_clean(frame_idx):\n",
        "    sec = frame_idx / fps\n",
        "    return any(start <= sec <= end for start, end in kezmentes_intervallumok)\n",
        "\n",
        "prompt_counts = []\n",
        "false_positives = []\n",
        "\n",
        "for i in range(1, len(additional_prompts) + 1):\n",
        "    print(f\"\\n {i+4} prompttal történő detekció...\")\n",
        "\n",
        "    all_prompts = clip_prompts + additional_prompts[:i]\n",
        "    tokenized = clip.tokenize(all_prompts).to(device)\n",
        "\n",
        "    fp_count = 0\n",
        "    checked_frames = 0\n",
        "\n",
        "    cap = cv2.VideoCapture(\"/content/drive/MyDrive/baba_legzes_2.mp4\")\n",
        "    frame_idx = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if not is_frame_clean(frame_idx):\n",
        "            frame_idx += 1\n",
        "            continue\n",
        "\n",
        "        detections, _ = run_yolo(model_v7, frame)\n",
        "        for det in detections:\n",
        "            x1, y1, x2, y2, conf, cls = det\n",
        "            if int(cls) == 0:\n",
        "                x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "                baby_crop = frame[y1:y2, x1:x2]\n",
        "                if baby_crop.size == 0:\n",
        "                    continue\n",
        "\n",
        "                image = preprocess(Image.fromarray(baby_crop)).unsqueeze(0).to(device)\n",
        "                with torch.no_grad():\n",
        "                    image_features = clip_model.encode_image(image)\n",
        "                    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "                    text_features = clip_model.encode_text(tokenized)\n",
        "                    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "                    similarity = (image_features @ text_features.T).squeeze(0)\n",
        "                    best_match_idx = similarity.argmax().item()\n",
        "                    best_prompt = all_prompts[best_match_idx]\n",
        "\n",
        "                    if \"hand\" in best_prompt or \"nurse\" in best_prompt:\n",
        "                        fp_count += 1\n",
        "                break\n",
        "        frame_idx += 1\n",
        "    cap.release()\n",
        "\n",
        "    prompt_counts.append(i + 4)\n",
        "    false_positives.append(fp_count)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(prompt_counts, false_positives, marker='o')\n",
        "plt.title(\"False positive detekciók száma a promptok számának függvényében\")\n",
        "plt.xlabel(\"CLIP promptok száma\")\n",
        "plt.ylabel(\"Téves 'kéz' detekciók (kézmentes szakaszokon)\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "4jN030R0GZe7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/baba_legzes_2.mp4\")\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "crop_size = 320\n",
        "out = cv2.VideoWriter(\"tiszta_baba_cropok_fixkeret_baba_feje_nem_latszik.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), fps, (crop_size, crop_size))\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    detections, _ = run_yolo(model_v7, frame)\n",
        "    baby_class = 0\n",
        "\n",
        "    for det in detections:\n",
        "        x1, y1, x2, y2, conf, cls = det\n",
        "        if int(cls) == baby_class:\n",
        "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "            baby_crop = frame[y1:y2, x1:x2]\n",
        "            if baby_crop.size == 0:\n",
        "                continue\n",
        "\n",
        "            has_hand = clip_detect_hand_near_baby(baby_crop)\n",
        "\n",
        "            if not has_hand:\n",
        "                cx = (x1 + x2) // 2\n",
        "                cy = (y1 + y2) // 2\n",
        "\n",
        "                half = crop_size // 2\n",
        "                x1_crop = max(0, cx - half)\n",
        "                y1_crop = max(0, cy - half)\n",
        "                x2_crop = x1_crop + crop_size\n",
        "                y2_crop = y1_crop + crop_size\n",
        "\n",
        "                if x2_crop > frame.shape[1]:\n",
        "                    x1_crop = frame.shape[1] - crop_size\n",
        "                    x2_crop = frame.shape[1]\n",
        "                if y2_crop > frame.shape[0]:\n",
        "                    y1_crop = frame.shape[0] - crop_size\n",
        "                    y2_crop = frame.shape[0]\n",
        "\n",
        "                fixed_crop = frame[y1_crop:y2_crop, x1_crop:x2_crop]\n",
        "\n",
        "                if fixed_crop.shape[:2] == (crop_size, crop_size):\n",
        "                    out.write(fixed_crop)\n",
        "\n",
        "            break\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(\" Elkészült a 'tiszta' baba cropokat tartalmazó videó: tiszta_baba_cropok_fixkeret.mp4\")\n"
      ],
      "metadata": {
        "id": "120vPWaRIe-P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cap = cv2.VideoCapture(\"/content/drive/MyDrive/baba_legzes_2.mp4\")\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "out = cv2.VideoWriter(\"tiszta_baba_cropok2_baba_1.mp4\", cv2.VideoWriter_fourcc(*'mp4v'), fps, (224, 224))\n",
        "frame_idx = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    detections, _ = run_yolo(model_v7, frame)\n",
        "    baby_class = 0\n",
        "    baby_found = False\n",
        "\n",
        "    for det in detections:\n",
        "        x1, y1, x2, y2, conf, cls = det\n",
        "        if int(cls) == baby_class:\n",
        "            x1, y1, x2, y2 = map(int, [x1, y1, x2, y2])\n",
        "            baby_crop = frame[y1:y2, x1:x2]\n",
        "            if baby_crop.size == 0:\n",
        "                continue\n",
        "\n",
        "            baby_found = True\n",
        "            has_hand = clip_detect_hand_near_baby(baby_crop)\n",
        "\n",
        "            if not has_hand:\n",
        "                resized_crop = cv2.resize(baby_crop, (224, 224))\n",
        "                out.write(resized_crop)\n",
        "\n",
        "            break\n",
        "    frame_idx += 1\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "print(\" Elkészült a 'tiszta' baba cropokat tartalmazó videó: tiszta_baba_cropok.mp4\")\n"
      ],
      "metadata": {
        "id": "BtLiP1qJnGFO"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}